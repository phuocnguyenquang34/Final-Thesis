{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Install latest version of the library\n",
    "!pip install -q datasets==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git --quiet\n",
    "!git clone https://github.com/drboog/Shifted_Diffusion.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Shifted_Diffusion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ./requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:38:31.976343Z",
     "iopub.status.busy": "2023-11-28T17:38:31.975589Z",
     "iopub.status.idle": "2023-11-28T17:38:31.984790Z",
     "shell.execute_reply": "2023-11-28T17:38:31.983890Z",
     "shell.execute_reply.started": "2023-11-28T17:38:31.976308Z"
    }
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    train_data_dir = \"/kaggle/input/mscoco-2017\"\n",
    "    mixed_precision = \"fp16\"\n",
    "    image_column = \"image\"\n",
    "    caption_column = \"text\"\n",
    "    output_dir = \"/kaggle/working/sft\"\n",
    "    seed = 1337\n",
    "    #random_flip\n",
    "    train_batch_size = 4\n",
    "    num_train_epochs = 30\n",
    "    max_train_steps = None\n",
    "    gradient_accumulation_steps = 4\n",
    "    gradient_checkpointing = True\n",
    "    #The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]'\n",
    "    lr_scheduler = \"constant\"\n",
    "    #Number of steps for the warmup in the lr scheduler.\n",
    "    lr_warmup_steps = 500\n",
    "    #Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\n",
    "    dataloader_num_workers = 0\n",
    "    #Save a checkpoint of the training state every X updates. These checkpoints are only suitable for resuming training using `--resume_from_checkpoint`.\n",
    "    checkpointing_steps = 5000\n",
    "    checkpoints_total_limit = None\n",
    "    resume_from_checkpoint = None\n",
    "    learning_rate = 1.2e-4\n",
    "    adam_beta1 = 0.9\n",
    "    adam_beta2 = 0.96\n",
    "    adam_weight_decay = 0.06\n",
    "    adam_epsilon = 1e-06\n",
    "    max_grad_norm = 1.0\n",
    "    noise_schedule = \"linear\"\n",
    "    t5_model = \"google/flan-t5-large\"\n",
    "    empty_t5_prob = 0.1\n",
    "    empty_clip_prob = 0.\n",
    "    use_vocab = True\n",
    "    vocab_learnable = False\n",
    "    exp = False\n",
    "    size = 1024\n",
    "    use_mean = True\n",
    "    sample_num = 1\n",
    "    mean_path = \"/kaggle/input/datafortrainingshifted/mean.pth\"\n",
    "    std_path = \"/kaggle/input/datafortrainingshifted/std.pth\"\n",
    "    beta_min = 0.0001\n",
    "    beta_max = 0.02\n",
    "    std_scale = 5.0\n",
    "    p2_gamma = 1.0\n",
    "    vocab_lr_scale = 0.01\n",
    "    model_width = 2048\n",
    "    model_layers = 8\n",
    "    model_num_heads = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:38:32.196434Z",
     "iopub.status.busy": "2023-11-28T17:38:32.196065Z",
     "iopub.status.idle": "2023-11-28T17:38:34.684265Z",
     "shell.execute_reply": "2023-11-28T17:38:34.683440Z",
     "shell.execute_reply.started": "2023-11-28T17:38:32.196398Z"
    }
   },
   "outputs": [],
   "source": [
    "import accelerate\n",
    "from accelerate import Accelerator, notebook_launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:38:34.686997Z",
     "iopub.status.busy": "2023-11-28T17:38:34.686492Z",
     "iopub.status.idle": "2023-11-28T17:38:34.748857Z",
     "shell.execute_reply": "2023-11-28T17:38:34.747995Z",
     "shell.execute_reply.started": "2023-11-28T17:38:34.686962Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    from pathlib import Path\n",
    "    import logging\n",
    "    from time import time\n",
    "    import datetime\n",
    "    import random\n",
    "    from tqdm import tqdm, trange\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import sys\n",
    "    import copy\n",
    "    import math\n",
    "    import torch\n",
    "    torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "    import transformers\n",
    "    import clip\n",
    "    from packaging import version\n",
    "    import datasets\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    from transformers import AutoTokenizer, T5EncoderModel, T5Config, T5ForConditionalGeneration\n",
    "    from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "\n",
    "    import accelerate\n",
    "    from accelerate import Accelerator, notebook_launcher\n",
    "    from accelerate.logging import get_logger\n",
    "    from accelerate.utils import ProjectConfiguration, set_seed\n",
    "    from diffusers.optimization import get_scheduler\n",
    "\n",
    "    from model_lib.decoder.clip_prior import ClipPrior, Vocab\n",
    "    from model_lib.diffusion.script_util import create_sft_gaussian_diffusion as create_gaussian_diffusion_p2\n",
    "    from model_lib.diffusion.resample import create_named_schedule_sampler as create_named_schedule_sampler_p2\n",
    "    #from utils.checkpoint import save_checkpoint, load_from_pretrain\n",
    "    \n",
    "    logger = get_logger(__name__, log_level=\"INFO\")\n",
    "\n",
    "    class MultiCLIP(torch.nn.Module):\n",
    "        def __init__(self, device=\"cpu\"):\n",
    "            super().__init__()\n",
    "            model_32, _ = clip.load(\"/kaggle/input/datafortrainingshifted/ViT-B-32.pt\", device=device)\n",
    "            model_16, _ = clip.load(\"/kaggle/input/datafortrainingshifted/ViT-B-16.pt\", device=device)\n",
    "            model_101, _ = clip.load(\"/kaggle/input/datafortrainingshifted/RN101.pt\", device=device)\n",
    "            self.model_32 = model_32\n",
    "            self.model_16 = model_16\n",
    "            self.model_101 = model_101\n",
    "            # self.preprocess = Compose([\n",
    "            #     Resize(224, interpolation=BICUBIC),\n",
    "            #     Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "            # ])\n",
    "\n",
    "        def encode_image(self, image, dtype):\n",
    "            with torch.no_grad():\n",
    "                # image = self.preprocess(image)\n",
    "                vectors = [self.model_16.encode_image(image.to(dtype)), self.model_32.encode_image(image.to(dtype)), self.model_101.encode_image(image.to(dtype))]\n",
    "                return torch.cat(vectors, dim=-1).to(dtype)\n",
    "\n",
    "        def encode_text(self, text, dtype, device):\n",
    "            with torch.no_grad():\n",
    "                text = clip.tokenize(text).to(device)\n",
    "                vectors = [self.model_16.encode_text(text), self.model_32.encode_text(text), self.model_101.encode_text(text)]\n",
    "                return torch.cat(vectors, dim=-1).to(dtype)\n",
    "\n",
    "    def convert_weights(model: torch.nn.Module):\n",
    "        \"\"\"Convert applicable model parameters to fp16\"\"\"\n",
    "\n",
    "        def _convert_weights_to_fp16(l):\n",
    "            if isinstance(l, (torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Linear)):\n",
    "                l.weight.data = l.weight.data.half()\n",
    "                if l.bias is not None:\n",
    "                    l.bias.data = l.bias.data.half()\n",
    "\n",
    "            if isinstance(l, torch.nn.MultiheadAttention):\n",
    "                for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n",
    "                    tensor = getattr(l, attr)\n",
    "                    if tensor is not None:\n",
    "                        tensor.data = tensor.data.half()\n",
    "\n",
    "            for name in [\"text_projection\", \"proj\"]:\n",
    "                if hasattr(l, name):\n",
    "                    attr = getattr(l, name)\n",
    "                    if attr is not None:\n",
    "                        attr.data = attr.data.half()\n",
    "\n",
    "        model.apply(_convert_weights_to_fp16)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_t5_embeddings(texts, t5_encoder, t5_tokenizer):\n",
    "        input_ids = t5_tokenizer( texts , return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=80).input_ids.to(t5_encoder.device)\n",
    "        outputs = t5_encoder( input_ids=input_ids ).last_hidden_state\n",
    "        return outputs # in shape: B x Seq_len x Hidden_size (1024)\n",
    "\n",
    "    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)\n",
    "    logging_dir = os.path.join(args.output_dir, \"logs\")\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        mixed_precision=args.mixed_precision,\n",
    "        project_config=accelerator_project_config,\n",
    "        project_dir=logging_dir,\n",
    "    )\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger.info(accelerator.state, main_process_only=False)\n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_warning()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "    if accelerator.is_main_process:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    log_std_init = torch.log(torch.load(args.std_path, map_location='cpu').view((-1, 1536)))[:args.size]\n",
    "    mean_init = torch.load(args.mean_path, map_location='cpu').view((-1, 1536))[:args.size]\n",
    "    # lg_loss_scale = args.initial_lg_loss_scale\n",
    "\n",
    "    logger.info(\" Loading pre-trained text encoders, may take some time if download is needed.\")\n",
    "    clip_model = MultiCLIP()\n",
    "    \n",
    "    weight_dtype = torch.float32\n",
    "    \n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "#         convert_weights(clip_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.t5_model, model_max_length=80)\n",
    "    t5_encoder = T5EncoderModel.from_pretrained(args.t5_model, low_cpu_mem_usage=True, torch_dtype=weight_dtype)\n",
    "    clip_model.requires_grad_(False)\n",
    "    t5_encoder.requires_grad_(False)\n",
    "\n",
    "    model = ClipPrior(xf_width=args.model_width, xf_layers=args.model_layers, xf_heads=args.model_num_heads,\n",
    "                      clip_width=512*3, learn_sigma=False, t5_dim=t5_encoder.config.d_model, use_vocab=args.use_vocab,\n",
    "                      vocab_size=args.size, vocab_use_mean=args.use_mean, vocab_sample_num=args.sample_num,\n",
    "                      vocab_log_std_init=log_std_init, vocab_mean_init=mean_init, vocab_learnable=args.vocab_learnable,\n",
    "                      vocab_std_scale=args.std_scale, vocab_lr_scale=args.vocab_lr_scale, vocab_exp=args.exp)\n",
    "\n",
    "    # if args.gradient_checkpointing: # TODO\n",
    "    #     model.enable_gradient_checkpointing()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        betas=(args.adam_beta1, args.adam_beta2),\n",
    "        eps=args.adam_epsilon,\n",
    "        weight_decay=args.adam_weight_decay\n",
    "    )\n",
    "\n",
    "    data_files = {}\n",
    "    if args.train_data_dir is not None:\n",
    "        data_files[\"train\"] = os.path.join(args.train_data_dir, \"**\")\n",
    "    dataset = load_dataset(\n",
    "        \"imagefolder\",\n",
    "        data_files=data_files,\n",
    "    )\n",
    "    column_names = dataset[\"train\"].column_names\n",
    "    image_column = args.image_column\n",
    "    if image_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "\n",
    "    if args.caption_column is None:\n",
    "        caption_column = column_names[1]\n",
    "    else:\n",
    "        caption_column = args.caption_column\n",
    "        if caption_column not in column_names:\n",
    "            raise ValueError(\n",
    "                f\"--caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "            )\n",
    "\n",
    "    def tokenize_captions(examples, is_train=True):\n",
    "        captions = []\n",
    "        for caption in examples[caption_column]:\n",
    "            if isinstance(caption, str):\n",
    "                captions.append(caption)\n",
    "            elif isinstance(caption, (list, np.ndarray)):\n",
    "                # take a random caption if there are multiple\n",
    "                captions.append(random.choice(caption) if is_train else caption[0])\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n",
    "                )\n",
    "        inputs = tokenizer(\n",
    "            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        return inputs.input_ids, captions\n",
    "\n",
    "    train_transforms = Compose(\n",
    "        [ToTensor(),\n",
    "         Resize(224, interpolation=BICUBIC),\n",
    "         CenterCrop(224),\n",
    "         Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "         ]\n",
    "    )\n",
    "\n",
    "    def preprocess_train(examples):\n",
    "        images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
    "        examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
    "        examples[\"input_ids\"], examples[\"text\"]  = tokenize_captions(examples)\n",
    "        return examples\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n",
    "\n",
    "    def collate_fn(examples):\n",
    "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "        text = [example[\"text\"] for example in examples]\n",
    "        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids, \"text\": text}\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=args.train_batch_size,\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "    )\n",
    "\n",
    "    overrode_max_train_steps = False\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    if args.max_train_steps is None:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "        overrode_max_train_steps = True\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        args.lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
    "        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",
    "    )\n",
    "\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "    clip_model.to(accelerator.device)\n",
    "    t5_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "    try:\n",
    "        vocab = model.module.vocab\n",
    "    except:\n",
    "        vocab = model.vocab\n",
    "\n",
    "    if not args.vocab_learnable:\n",
    "        vocab.mean = mean_init.to(accelerator.device)\n",
    "        vocab.std = log_std_init.exp().to(accelerator.device)\n",
    "\n",
    "    diffusion = create_gaussian_diffusion_p2(steps=1000,\n",
    "                                            learn_sigma=False,\n",
    "                                            noise_schedule=args.noise_schedule,\n",
    "                                            use_kl=False,\n",
    "                                            predict_xstart=True,\n",
    "                                            predict_prev=False,\n",
    "                                            rescale_timesteps=False,\n",
    "                                            rescale_learned_sigmas=False,\n",
    "                                            timestep_respacing=\"\",\n",
    "                                            p2_gamma=args.p2_gamma,\n",
    "                                            p2_k=1,\n",
    "                                            vocab = vocab,\n",
    "                                            beta_min=args.beta_min,\n",
    "                                            beta_max=args.beta_max,\n",
    "                                             )\n",
    "    schedule_sampler = create_named_schedule_sampler_p2(\"uniform\", diffusion)\n",
    "\n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    if overrode_max_train_steps:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    # Afterwards we recalculate our number of training epochs\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(\"prior\", config=vars(args))\n",
    "\n",
    "    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "    global_step = 0\n",
    "    first_epoch = 0\n",
    "\n",
    "    if args.resume_from_checkpoint:\n",
    "        if args.resume_from_checkpoint != \"latest\":\n",
    "            path = os.path.basename(args.resume_from_checkpoint)\n",
    "        else:\n",
    "            # Get the most recent checkpoint\n",
    "            dirs = os.listdir(args.output_dir)\n",
    "            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "            path = dirs[-1] if len(dirs) > 0 else None\n",
    "\n",
    "        if path is None:\n",
    "            accelerator.print(\n",
    "                f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
    "            )\n",
    "            args.resume_from_checkpoint = None\n",
    "        else:\n",
    "            accelerator.print(f\"Resuming from checkpoint {path}\")\n",
    "            accelerator.load_state(os.path.join(args.output_dir, path))\n",
    "            global_step = int(path.split(\"-\")[1])\n",
    "\n",
    "            resume_global_step = global_step * args.gradient_accumulation_steps\n",
    "            first_epoch = global_step // num_update_steps_per_epoch\n",
    "            resume_step = resume_global_step % (num_update_steps_per_epoch * args.gradient_accumulation_steps)\n",
    "\n",
    "    progress_bar = tqdm(range(global_step, args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    progress_bar.set_description(\"Steps\")\n",
    "\n",
    "    for epoch in range(first_epoch, args.num_train_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if args.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\n",
    "                if step % args.gradient_accumulation_steps == 0:\n",
    "                    progress_bar.update(1)\n",
    "                continue\n",
    "            with accelerator.accumulate(model):\n",
    "                clip_image_emb = clip_model.encode_image(batch[\"pixel_values\"], dtype=weight_dtype)\n",
    "                clip_text_emb = clip_model.encode_text(batch[\"text\"], dtype=weight_dtype, device=accelerator.device)\n",
    "                # input_t5_emb = get_t5_embeddings(text, t5_encoder, t5_tokenizer)\n",
    "                input_t5_emb = t5_encoder(input_ids=batch[\"input_ids\"]).last_hidden_state\n",
    "\n",
    "                input_clip_emb_modified = clip_text_emb.detach().clone()\n",
    "                clip_empty_idx = torch.rand(clip_image_emb.shape[0]) <= args.empty_clip_prob\n",
    "                input_clip_emb_modified[clip_empty_idx] *= 0.0\n",
    "\n",
    "                input_t5_emb_modified = input_t5_emb.detach().clone()\n",
    "                t5_empty_idx = torch.rand(clip_image_emb.shape[0]) <= args.empty_t5_prob\n",
    "                input_t5_emb_modified[t5_empty_idx] *= 0.0\n",
    "\n",
    "                t, weights = schedule_sampler.sample(clip_image_emb.shape[0], accelerator.device, p=None, weights_np=None) # weights shape (batch_size,)\n",
    "                losses = diffusion.training_losses(model, clip_image_emb, t, #emb_4_vocab=clip_image_emb,\n",
    "                                                   model_kwargs=dict(emb_4_vocab=clip_text_emb, #clip_image_emb,\n",
    "                                                    t5_word_emb=input_t5_emb_modified,\n",
    "                                                    clip_sentence_emb=input_clip_emb_modified\n",
    "                                                    ), use_d=False, discriminator=None)\n",
    "                loss = (losses[\"loss\"] * weights).mean()\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
    "                train_loss = 0.0\n",
    "                if global_step % args.checkpointing_steps == 0:\n",
    "                    if accelerator.is_main_process:\n",
    "                        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "                        accelerator.save_state(save_path)\n",
    "                        logger.info(f\"Saved state to {save_path}\")\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "    # # Create the pipeline using the trained modules and save it.\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        model = accelerator.unwrap_model(model)\n",
    "        accelerator.save(model.state_dict(), os.path.join(args.output_dir, 'prior.pt'))\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:59:17.957847Z",
     "iopub.status.busy": "2023-11-28T17:59:17.957471Z",
     "iopub.status.idle": "2023-11-28T17:59:17.962618Z",
     "shell.execute_reply": "2023-11-28T17:59:17.961613Z",
     "shell.execute_reply.started": "2023-11-28T17:59:17.957815Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"Shifted_Diffusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T17:59:18.935272Z",
     "iopub.status.busy": "2023-11-28T17:59:18.934864Z"
    }
   },
   "outputs": [],
   "source": [
    "notebook_launcher(main, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4052466,
     "sourceId": 7043101,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4066162,
     "sourceId": 7062676,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
